{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf7bfc0-4c45-4339-9851-226039159d0b",
   "metadata": {},
   "source": [
    "tf-idf:\n",
    "tf-idf is a word wheightning technique which gives higher weights to the least frequent words which are significant but less frequent in the corpus.\n",
    "like a corpus related to space has a word 'hubble telescope' only twice in the corpus, so in traditional techniques hubble telescope is less important, \n",
    "tf-idf make this more significant.\n",
    "tf-idf=log(tf+1)*log(N/df), here tf is the term frequency in given corpus , N is total number of documents in the collection and df is the number of documents\n",
    "that has the given words.\n",
    "1 is added because log(0) is undefined.\n",
    "according to tf-idf a word presenting in all documents have zero tf-idf value\n",
    "higher the tf-idf value more significance it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13961441-16c3-47a7-93cf-e955b543a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8286b9d-db36-4cf7-ad23-267dd495d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to clean the raw text before finding count vectorizer\n",
    "def cleaning(text):\n",
    "    sentence=nltk.sent_tokenize(text)\n",
    "    lematize=WordNetLemmatizer()\n",
    "    for i in range(len(sentence)):\n",
    "        words=re.sub('[^a-zA-Z]',' ',sentence[i])\n",
    "        words=words.lower()\n",
    "        words=words.split()\n",
    "        temp=[]\n",
    "        for word in words:\n",
    "            if word not in stopwords.words('english'):\n",
    "                lem=lematize.lemmatize(word)\n",
    "                temp.append(lem)\n",
    "            sentence[i]=' '.join(temp)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb4fe14-e52b-4a8c-8696-bcf26a03bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(text):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vector=TfidfVectorizer()\n",
    "    array=vector.fit_transform(text).toarray()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf2466e-8aa0-4d5d-9a2b-39f07f1bdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Stemming is a fundamental technique in Natural Language Processing (NLP) used to reduce words to their root or base form, aiming to normalize text for analysis. For instance, applying stemming to the words \"running\", \"runs\", and \"runner\" would result in the root \"run\". This process helps in condensing variations of words to a common form, which is crucial for tasks like text classification, information retrieval, and sentiment analysis. Despite its simplicity, stemming algorithms like Porter and Snowball can sometimes produce stems that are not actual words, which may affect accuracy in certain contexts. Nevertheless, stemming remains a valuable preprocessing step in many NLP applications, contributing to the efficiency and effectiveness of text processing pipelines.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25120f87-2752-42f2-ab47-71805065cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text=cleaning(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90d4751-2f2d-412d-ae94-9ce06cd2ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "array=tf_idf(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b92e1449-52bd-4f0c-8233-2b297a922c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf values are :\n",
      "[[0.         0.         0.         0.27381445 0.         0.22091178\n",
      "  0.         0.         0.27381445 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.22091178 0.27381445 0.         0.         0.\n",
      "  0.27381445 0.         0.         0.         0.27381445 0.\n",
      "  0.22091178 0.27381445 0.         0.         0.         0.\n",
      "  0.22091178 0.         0.27381445 0.         0.         0.\n",
      "  0.22091178 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.15426229 0.         0.\n",
      "  0.27381445 0.18337673 0.27381445 0.         0.         0.15426229\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29767026 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.29767026\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.29767026 0.\n",
      "  0.24015849 0.59534052 0.29767026 0.29767026 0.         0.\n",
      "  0.         0.         0.         0.16770223 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.16770223\n",
      "  0.29767026]\n",
      " [0.         0.         0.         0.         0.         0.21782445\n",
      "  0.         0.         0.         0.         0.26998779 0.26998779\n",
      "  0.26998779 0.         0.         0.26998779 0.         0.\n",
      "  0.         0.21782445 0.         0.26998779 0.26998779 0.\n",
      "  0.         0.21782445 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26998779\n",
      "  0.         0.         0.         0.         0.         0.26998779\n",
      "  0.         0.         0.         0.         0.26998779 0.\n",
      "  0.         0.         0.         0.         0.         0.26998779\n",
      "  0.         0.18081397 0.         0.         0.26998779 0.15210641\n",
      "  0.        ]\n",
      " [0.25577442 0.25577442 0.25577442 0.         0.25577442 0.\n",
      "  0.         0.         0.         0.25577442 0.         0.\n",
      "  0.         0.25577442 0.         0.         0.25577442 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.20635719 0.         0.25577442 0.         0.\n",
      "  0.         0.         0.         0.25577442 0.         0.\n",
      "  0.         0.25577442 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25577442\n",
      "  0.25577442 0.25577442 0.25577442 0.14409885 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.14409885\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.27663025 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27663025 0.         0.         0.27663025\n",
      "  0.27663025 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27663025 0.         0.         0.27663025\n",
      "  0.22318354 0.         0.27663025 0.         0.27663025 0.\n",
      "  0.22318354 0.         0.         0.27663025 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.15584865 0.27663025 0.\n",
      "  0.         0.1852625  0.         0.27663025 0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"tf-idf values are :\")\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32d1747e-2dc7-4d02-b2df-825d0b27dd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1535adf4-f19f-487e-834b-ee81d82c2cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 61)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26700dfe-5106-44c1-98aa-f68d1ef5fcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 2., 1., 2., 1., 3., 2., 1., 1., 1., 3., 1., 3., 3., 1.,\n",
       "        1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 3., 2., 1., 2., 2.,\n",
       "        3., 2., 2., 1., 2., 1., 2., 2., 2., 2., 3., 1., 3., 1., 2., 1.,\n",
       "        2., 2., 1., 2., 2., 2., 1., 3., 3., 1., 2., 3., 2.],\n",
       "       [2., 2., 2., 3., 3., 1., 2., 3., 1., 2., 2., 1., 1., 1., 2., 3.,\n",
       "        2., 3., 1., 3., 2., 1., 2., 1., 3., 2., 3., 1., 1., 1., 2., 2.,\n",
       "        1., 1., 3., 1., 2., 2., 1., 3., 2., 3., 3., 2., 3., 1., 1., 3.,\n",
       "        2., 3., 2., 2., 1., 2., 2., 3., 3., 1., 3., 1., 3.],\n",
       "       [1., 3., 3., 1., 1., 3., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3.,\n",
       "        1., 1., 1., 3., 1., 1., 1., 3., 3., 3., 3., 3., 3., 2., 2., 2.,\n",
       "        2., 2., 1., 3., 3., 1., 2., 3., 3., 1., 2., 1., 1., 1., 2., 3.,\n",
       "        3., 3., 1., 3., 1., 1., 3., 3., 1., 3., 1., 1., 3.],\n",
       "       [2., 2., 1., 3., 3., 2., 2., 2., 1., 3., 3., 3., 3., 2., 3., 2.,\n",
       "        2., 3., 2., 1., 3., 1., 2., 3., 1., 2., 3., 2., 2., 2., 1., 2.,\n",
       "        3., 1., 2., 2., 2., 1., 3., 3., 3., 1., 2., 3., 3., 2., 1., 1.,\n",
       "        2., 1., 2., 3., 2., 2., 3., 3., 1., 3., 2., 1., 2.],\n",
       "       [1., 1., 3., 1., 3., 2., 1., 2., 2., 2., 1., 3., 3., 3., 1., 2.,\n",
       "        2., 1., 2., 2., 3., 1., 3., 2., 2., 2., 3., 3., 3., 3., 2., 1.,\n",
       "        2., 1., 2., 1., 1., 2., 3., 1., 3., 3., 1., 2., 2., 1., 1., 3.,\n",
       "        2., 1., 3., 1., 1., 3., 1., 2., 3., 1., 2., 1., 2.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we have successfully convert words into arrays so we can perform arthimitic operations\n",
    "import numpy as np\n",
    "arry=np.random.randint(1,4,size=(5,61))\n",
    "arry=np.array(arry)\n",
    "arry=arry.astype(float)\n",
    "arry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2eac4bf6-2c95-4a99-9e7b-4ab81da83921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 61)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arry.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4d592fe-3c30-4878-991a-4487fbeefd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of tf-idf array and numpy array of same size is \n",
      "[[3.         3.         2.         1.27381445 2.         1.22091178\n",
      "  3.         2.         1.27381445 1.         1.         3.\n",
      "  1.         3.         3.         1.         1.         1.\n",
      "  1.         1.22091178 2.27381445 2.         2.         2.\n",
      "  2.27381445 2.         2.         3.         2.27381445 1.\n",
      "  2.22091178 2.27381445 3.         2.         2.         1.\n",
      "  2.22091178 1.         2.27381445 2.         2.         2.\n",
      "  3.22091178 1.         3.         1.         2.         1.\n",
      "  2.         2.         1.         2.15426229 2.         2.\n",
      "  1.27381445 3.18337673 3.27381445 1.         2.         3.15426229\n",
      "  2.        ]\n",
      " [2.         2.         2.         3.         3.         1.\n",
      "  2.         3.29767026 1.         2.         2.         1.\n",
      "  1.         1.         2.         3.         2.         3.\n",
      "  1.         3.         2.         1.         2.         1.29767026\n",
      "  3.         2.         3.         1.         1.         1.\n",
      "  2.         2.         1.         1.         3.         1.\n",
      "  2.         2.         1.         3.         2.29767026 3.\n",
      "  3.24015849 2.59534052 3.29767026 1.29767026 1.         3.\n",
      "  2.         3.         2.         2.16770223 1.         2.\n",
      "  2.         3.         3.         1.         3.         1.16770223\n",
      "  3.29767026]\n",
      " [1.         3.         3.         1.         1.         3.21782445\n",
      "  3.         1.         1.         2.         3.26998779 3.26998779\n",
      "  3.26998779 2.         3.         3.26998779 1.         1.\n",
      "  1.         3.21782445 1.         1.26998779 1.26998779 3.\n",
      "  3.         3.21782445 3.         3.         3.         2.\n",
      "  2.         2.         2.         2.         1.         3.26998779\n",
      "  3.         1.         2.         3.         3.         1.26998779\n",
      "  2.         1.         1.         1.         2.26998779 3.\n",
      "  3.         3.         1.         3.         1.         1.26998779\n",
      "  3.         3.18081397 1.         3.         1.26998779 1.15210641\n",
      "  3.        ]\n",
      " [2.25577442 2.25577442 1.25577442 3.         3.25577442 2.\n",
      "  2.         2.         1.         3.25577442 3.         3.\n",
      "  3.         2.25577442 3.         2.         2.25577442 3.\n",
      "  2.         1.         3.         1.         2.         3.\n",
      "  1.         2.20635719 3.         2.25577442 2.         2.\n",
      "  1.         2.         3.         1.25577442 2.         2.\n",
      "  2.         1.25577442 3.         3.         3.         1.\n",
      "  2.         3.         3.         2.         1.         1.25577442\n",
      "  2.25577442 1.25577442 2.25577442 3.14409885 2.         2.\n",
      "  3.         3.         1.         3.         2.         1.14409885\n",
      "  2.        ]\n",
      " [1.         1.         3.         1.         3.         2.\n",
      "  1.27663025 2.         2.         2.         1.         3.\n",
      "  3.         3.         1.27663025 2.         2.         1.27663025\n",
      "  2.27663025 2.         3.         1.         3.         2.\n",
      "  2.         2.         3.27663025 3.         3.         3.27663025\n",
      "  2.22318354 1.         2.27663025 1.         2.27663025 1.\n",
      "  1.22318354 2.         3.         1.27663025 3.         3.\n",
      "  1.         2.         2.         1.         1.         3.\n",
      "  2.         1.         3.         1.15584865 1.27663025 3.\n",
      "  1.         2.1852625  3.         1.27663025 2.         1.\n",
      "  2.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"sum of tf-idf array and numpy array of same size is \")\n",
    "print(arry+array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eacf58e-b112-451f-a909-76db918d6647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
