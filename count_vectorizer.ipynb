{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6098eaaa-4a99-44cc-9114-9e3ccbc0f90f",
   "metadata": {},
   "source": [
    "word to arrays:\n",
    "computer only understands numbers so we have to convert our textual input to numbers , this technique is called as word to array\n",
    "count vectorizer is used to convert words to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b62199b-28fc-4099-9857-d57f161606e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db52b63c-6324-4778-8dfe-6ea8692be171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to clean the raw text before finding count vectorizer\n",
    "def cleaning(text):\n",
    "    sentence=nltk.sent_tokenize(text)\n",
    "    lematize=WordNetLemmatizer()\n",
    "    for i in range(len(sentence)):\n",
    "        words=re.sub('[^a-zA-Z]',' ',sentence[i])\n",
    "        words=words.lower()\n",
    "        words=words.split()\n",
    "        temp=[]\n",
    "        for word in words:\n",
    "            if word not in stopwords.words('english'):\n",
    "                lem=lematize.lemmatize(word)\n",
    "                temp.append(lem)\n",
    "            sentence[i]=' '.join(temp)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b316461-ae36-47d2-9a50-4d90020d6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array(text):\n",
    "    cv=CountVectorizer()\n",
    "    array=cv.fit_transform(text).toarray()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "750873d5-cb7b-451d-99ec-fc5390299832",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Lemmatization is a crucial technique in Natural Language Processing (NLP) that aims to transform words into their base or dictionary form, known as the lemma. Unlike stemming, which simply chops off prefixes and suffixes to produce a root form that may not always be a valid word, lemmatization considers the context and grammatical structure of words to ensure accuracy. This process involves mapping words back to their canonical forms based on linguistic rules and dictionaries, thereby enhancing the quality and interpretability of textual analysis. For example, lemmatization would correctly reduce \"running\" to \"run\", \"better\" to \"good\", and \"mice\" to \"mouse\", making it indispensable in tasks like text classification, sentiment analysis, and machine translation. Despite being computationally more intensive than stemming, lemmatization provides more meaningful insights by preserving the semantic meaning of words, which is essential for extracting precise information from text data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed6b4efd-e62e-4276-8175-b5e7c6f1ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text=cleaning(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723d587b-1093-4e4e-a287-cda362cc394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=array(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60dada61-0bcf-4e28-947b-0c625fe0df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of text is\n",
      "[[0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1\n",
      "  1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  1 0 0 0 1 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
      "  0 0 1 1 2 0]\n",
      " [0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0\n",
      "  0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      "  0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      "  1 1 0 1 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0\n",
      "  0 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0\n",
      "  0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"array of text is\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292e1f3-81b4-4ea8-922e-3f8829195472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
